{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVDn2qmFvWQY6EeO7VBGWC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carol-alcan/redeNeuralMLP/blob/main/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WQoKX-e0pph"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('credit_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8C3bH40w2VV8",
        "outputId": "2d416755-1a80-4889-8485-4a934ea077e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   clientid        income        age         loan  default\n",
              "0         1  66155.925095  59.017015  8106.532131        0\n",
              "1         2  34415.153966  48.117153  6564.745018        0\n",
              "2         3  57317.170063  63.108049  8020.953296        0\n",
              "3         4  42709.534201  45.751972  6103.642260        0\n",
              "4         5  66952.688845  18.584336  8770.099235        1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-963c41d6-ad65-4af4-9655-a5c1cbe3f774\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clientid</th>\n",
              "      <th>income</th>\n",
              "      <th>age</th>\n",
              "      <th>loan</th>\n",
              "      <th>default</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>66155.925095</td>\n",
              "      <td>59.017015</td>\n",
              "      <td>8106.532131</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>34415.153966</td>\n",
              "      <td>48.117153</td>\n",
              "      <td>6564.745018</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>57317.170063</td>\n",
              "      <td>63.108049</td>\n",
              "      <td>8020.953296</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>42709.534201</td>\n",
              "      <td>45.751972</td>\n",
              "      <td>6103.642260</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>66952.688845</td>\n",
              "      <td>18.584336</td>\n",
              "      <td>8770.099235</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-963c41d6-ad65-4af4-9655-a5c1cbe3f774')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-963c41d6-ad65-4af4-9655-a5c1cbe3f774 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-963c41d6-ad65-4af4-9655-a5c1cbe3f774');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "crGQ6bq4cOW5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI9cRODx33VB",
        "outputId": "58eeda71-20f7-469b-81a9-82a9def33931"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1997, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_credit = df[['income', 'age', 'loan']]\n",
        "y_credit = df['default']"
      ],
      "metadata": {
        "id": "RjAR8xFs3--R"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "standard_scaler = StandardScaler()\n",
        "X_credit = standard_scaler.fit_transform(X_credit)"
      ],
      "metadata": {
        "id": "_2Y_70jo5Mrg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_credit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOWqWzCf6-Kd",
        "outputId": "185cea82-3231-49b7-bab3-7315d61344bc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.45389775,  1.33686061,  1.20190707],\n",
              "       [-0.76239757,  0.53663921,  0.69574418],\n",
              "       [ 0.8367328 ,  1.63720692,  1.17381186],\n",
              "       ...,\n",
              "       [-0.07139   , -0.93901609,  0.35367319],\n",
              "       [-0.11017022,  1.7006195 , -0.92670314],\n",
              "       [ 1.68296904,  1.12656872,  0.96300639]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_credit_treinamento, X_credit_teste, y_credit_treinamento, y_credit_teste = train_test_split(X_credit, y_credit, test_size= 0.25,random_state= 0)"
      ],
      "metadata": {
        "id": "5lUUj8tm8MyK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_credit_treinamento.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RjYmJUs-FrX",
        "outputId": "1a37655e-a07c-4ecd-9ae3-59d1a20ca1fe"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1497, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_credit_treinamento.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P2yUABz-fkS",
        "outputId": "95e69c0c-e2eb-45d2-91c6-6f207c0222da"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1497,)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_credit_teste.shape, y_credit_teste.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjNA2qCp-zl8",
        "outputId": "ea864ec5-6af7-457c-96b3-21196cca9efb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((500, 3), (500,))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "rede_neural_credit = MLPClassifier(max_iter = 1000,verbose = True, tol=0.0000100,\n",
        "                                   hidden_layer_sizes=(20,100))\n",
        "rede_neural_credit.fit(X_credit_treinamento, y_credit_treinamento)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q6OAqzDnadUd",
        "outputId": "ecfa2fdf-4f69-4e87-d26c-c33cdeb7a95b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.83444852\n",
            "Iteration 2, loss = 0.71164247\n",
            "Iteration 3, loss = 0.61810965\n",
            "Iteration 4, loss = 0.54015859\n",
            "Iteration 5, loss = 0.47526348\n",
            "Iteration 6, loss = 0.42106545\n",
            "Iteration 7, loss = 0.37582268\n",
            "Iteration 8, loss = 0.33630364\n",
            "Iteration 9, loss = 0.30377330\n",
            "Iteration 10, loss = 0.27523053\n",
            "Iteration 11, loss = 0.24976926\n",
            "Iteration 12, loss = 0.22824609\n",
            "Iteration 13, loss = 0.20915099\n",
            "Iteration 14, loss = 0.19249769\n",
            "Iteration 15, loss = 0.17768837\n",
            "Iteration 16, loss = 0.16453870\n",
            "Iteration 17, loss = 0.15269814\n",
            "Iteration 18, loss = 0.14227650\n",
            "Iteration 19, loss = 0.13323562\n",
            "Iteration 20, loss = 0.12486695\n",
            "Iteration 21, loss = 0.11814332\n",
            "Iteration 22, loss = 0.11142577\n",
            "Iteration 23, loss = 0.10559892\n",
            "Iteration 24, loss = 0.10034045\n",
            "Iteration 25, loss = 0.09570647\n",
            "Iteration 26, loss = 0.09133560\n",
            "Iteration 27, loss = 0.08700607\n",
            "Iteration 28, loss = 0.08315116\n",
            "Iteration 29, loss = 0.07946428\n",
            "Iteration 30, loss = 0.07582426\n",
            "Iteration 31, loss = 0.07265739\n",
            "Iteration 32, loss = 0.06978987\n",
            "Iteration 33, loss = 0.06655553\n",
            "Iteration 34, loss = 0.06406312\n",
            "Iteration 35, loss = 0.06168582\n",
            "Iteration 36, loss = 0.05965844\n",
            "Iteration 37, loss = 0.05790584\n",
            "Iteration 38, loss = 0.05568394\n",
            "Iteration 39, loss = 0.05363168\n",
            "Iteration 40, loss = 0.05235269\n",
            "Iteration 41, loss = 0.05054084\n",
            "Iteration 42, loss = 0.04921120\n",
            "Iteration 43, loss = 0.04786464\n",
            "Iteration 44, loss = 0.04680274\n",
            "Iteration 45, loss = 0.04570320\n",
            "Iteration 46, loss = 0.04426819\n",
            "Iteration 47, loss = 0.04312619\n",
            "Iteration 48, loss = 0.04244655\n",
            "Iteration 49, loss = 0.04101565\n",
            "Iteration 50, loss = 0.04047557\n",
            "Iteration 51, loss = 0.03950418\n",
            "Iteration 52, loss = 0.03878342\n",
            "Iteration 53, loss = 0.03792772\n",
            "Iteration 54, loss = 0.03739076\n",
            "Iteration 55, loss = 0.03636722\n",
            "Iteration 56, loss = 0.03559035\n",
            "Iteration 57, loss = 0.03525405\n",
            "Iteration 58, loss = 0.03457367\n",
            "Iteration 59, loss = 0.03371029\n",
            "Iteration 60, loss = 0.03315030\n",
            "Iteration 61, loss = 0.03276499\n",
            "Iteration 62, loss = 0.03200028\n",
            "Iteration 63, loss = 0.03169563\n",
            "Iteration 64, loss = 0.03103361\n",
            "Iteration 65, loss = 0.03049323\n",
            "Iteration 66, loss = 0.03010958\n",
            "Iteration 67, loss = 0.02964493\n",
            "Iteration 68, loss = 0.02910616\n",
            "Iteration 69, loss = 0.02882487\n",
            "Iteration 70, loss = 0.02843058\n",
            "Iteration 71, loss = 0.02790680\n",
            "Iteration 72, loss = 0.02758020\n",
            "Iteration 73, loss = 0.02726204\n",
            "Iteration 74, loss = 0.02702265\n",
            "Iteration 75, loss = 0.02696379\n",
            "Iteration 76, loss = 0.02623149\n",
            "Iteration 77, loss = 0.02594288\n",
            "Iteration 78, loss = 0.02555398\n",
            "Iteration 79, loss = 0.02515833\n",
            "Iteration 80, loss = 0.02492229\n",
            "Iteration 81, loss = 0.02451014\n",
            "Iteration 82, loss = 0.02433166\n",
            "Iteration 83, loss = 0.02399976\n",
            "Iteration 84, loss = 0.02378055\n",
            "Iteration 85, loss = 0.02349295\n",
            "Iteration 86, loss = 0.02325587\n",
            "Iteration 87, loss = 0.02295756\n",
            "Iteration 88, loss = 0.02286696\n",
            "Iteration 89, loss = 0.02238588\n",
            "Iteration 90, loss = 0.02226172\n",
            "Iteration 91, loss = 0.02186058\n",
            "Iteration 92, loss = 0.02161871\n",
            "Iteration 93, loss = 0.02141468\n",
            "Iteration 94, loss = 0.02108193\n",
            "Iteration 95, loss = 0.02119764\n",
            "Iteration 96, loss = 0.02083861\n",
            "Iteration 97, loss = 0.02055188\n",
            "Iteration 98, loss = 0.02036633\n",
            "Iteration 99, loss = 0.01996596\n",
            "Iteration 100, loss = 0.01993060\n",
            "Iteration 101, loss = 0.01973260\n",
            "Iteration 102, loss = 0.01970684\n",
            "Iteration 103, loss = 0.01925376\n",
            "Iteration 104, loss = 0.01922749\n",
            "Iteration 105, loss = 0.01906250\n",
            "Iteration 106, loss = 0.01867824\n",
            "Iteration 107, loss = 0.01852215\n",
            "Iteration 108, loss = 0.01841507\n",
            "Iteration 109, loss = 0.01813870\n",
            "Iteration 110, loss = 0.01790055\n",
            "Iteration 111, loss = 0.01766388\n",
            "Iteration 112, loss = 0.01765534\n",
            "Iteration 113, loss = 0.01781081\n",
            "Iteration 114, loss = 0.01721168\n",
            "Iteration 115, loss = 0.01719314\n",
            "Iteration 116, loss = 0.01693675\n",
            "Iteration 117, loss = 0.01673115\n",
            "Iteration 118, loss = 0.01650183\n",
            "Iteration 119, loss = 0.01631046\n",
            "Iteration 120, loss = 0.01622643\n",
            "Iteration 121, loss = 0.01598222\n",
            "Iteration 122, loss = 0.01594305\n",
            "Iteration 123, loss = 0.01566804\n",
            "Iteration 124, loss = 0.01557664\n",
            "Iteration 125, loss = 0.01539185\n",
            "Iteration 126, loss = 0.01524822\n",
            "Iteration 127, loss = 0.01511902\n",
            "Iteration 128, loss = 0.01507139\n",
            "Iteration 129, loss = 0.01497623\n",
            "Iteration 130, loss = 0.01461159\n",
            "Iteration 131, loss = 0.01450633\n",
            "Iteration 132, loss = 0.01442205\n",
            "Iteration 133, loss = 0.01443674\n",
            "Iteration 134, loss = 0.01423620\n",
            "Iteration 135, loss = 0.01413503\n",
            "Iteration 136, loss = 0.01390613\n",
            "Iteration 137, loss = 0.01390613\n",
            "Iteration 138, loss = 0.01394670\n",
            "Iteration 139, loss = 0.01355102\n",
            "Iteration 140, loss = 0.01356215\n",
            "Iteration 141, loss = 0.01331864\n",
            "Iteration 142, loss = 0.01341321\n",
            "Iteration 143, loss = 0.01295121\n",
            "Iteration 144, loss = 0.01294595\n",
            "Iteration 145, loss = 0.01281659\n",
            "Iteration 146, loss = 0.01256626\n",
            "Iteration 147, loss = 0.01265849\n",
            "Iteration 148, loss = 0.01243322\n",
            "Iteration 149, loss = 0.01235040\n",
            "Iteration 150, loss = 0.01208863\n",
            "Iteration 151, loss = 0.01212572\n",
            "Iteration 152, loss = 0.01219319\n",
            "Iteration 153, loss = 0.01172167\n",
            "Iteration 154, loss = 0.01196238\n",
            "Iteration 155, loss = 0.01163663\n",
            "Iteration 156, loss = 0.01149488\n",
            "Iteration 157, loss = 0.01129547\n",
            "Iteration 158, loss = 0.01134445\n",
            "Iteration 159, loss = 0.01126086\n",
            "Iteration 160, loss = 0.01118461\n",
            "Iteration 161, loss = 0.01092971\n",
            "Iteration 162, loss = 0.01082221\n",
            "Iteration 163, loss = 0.01095216\n",
            "Iteration 164, loss = 0.01064419\n",
            "Iteration 165, loss = 0.01059499\n",
            "Iteration 166, loss = 0.01063146\n",
            "Iteration 167, loss = 0.01064606\n",
            "Iteration 168, loss = 0.01044636\n",
            "Iteration 169, loss = 0.01031515\n",
            "Iteration 170, loss = 0.01032856\n",
            "Iteration 171, loss = 0.01009228\n",
            "Iteration 172, loss = 0.01012709\n",
            "Iteration 173, loss = 0.00986360\n",
            "Iteration 174, loss = 0.00982563\n",
            "Iteration 175, loss = 0.00975225\n",
            "Iteration 176, loss = 0.00971985\n",
            "Iteration 177, loss = 0.00990588\n",
            "Iteration 178, loss = 0.00949364\n",
            "Iteration 179, loss = 0.00941213\n",
            "Iteration 180, loss = 0.00951648\n",
            "Iteration 181, loss = 0.00919153\n",
            "Iteration 182, loss = 0.00925088\n",
            "Iteration 183, loss = 0.00922987\n",
            "Iteration 184, loss = 0.00930892\n",
            "Iteration 185, loss = 0.00907685\n",
            "Iteration 186, loss = 0.00905666\n",
            "Iteration 187, loss = 0.00909413\n",
            "Iteration 188, loss = 0.00890767\n",
            "Iteration 189, loss = 0.00869394\n",
            "Iteration 190, loss = 0.00874630\n",
            "Iteration 191, loss = 0.00859170\n",
            "Iteration 192, loss = 0.00866991\n",
            "Iteration 193, loss = 0.00876758\n",
            "Iteration 194, loss = 0.00842000\n",
            "Iteration 195, loss = 0.00843541\n",
            "Iteration 196, loss = 0.00819561\n",
            "Iteration 197, loss = 0.00817568\n",
            "Iteration 198, loss = 0.00800390\n",
            "Iteration 199, loss = 0.00809646\n",
            "Iteration 200, loss = 0.00816852\n",
            "Iteration 201, loss = 0.00785036\n",
            "Iteration 202, loss = 0.00797234\n",
            "Iteration 203, loss = 0.00786551\n",
            "Iteration 204, loss = 0.00777790\n",
            "Iteration 205, loss = 0.00772197\n",
            "Iteration 206, loss = 0.00776048\n",
            "Iteration 207, loss = 0.00758596\n",
            "Iteration 208, loss = 0.00754502\n",
            "Iteration 209, loss = 0.00798177\n",
            "Iteration 210, loss = 0.00729885\n",
            "Iteration 211, loss = 0.00779608\n",
            "Iteration 212, loss = 0.00733684\n",
            "Iteration 213, loss = 0.00729234\n",
            "Iteration 214, loss = 0.00720712\n",
            "Iteration 215, loss = 0.00739658\n",
            "Iteration 216, loss = 0.00711667\n",
            "Iteration 217, loss = 0.00731034\n",
            "Iteration 218, loss = 0.00701813\n",
            "Iteration 219, loss = 0.00728139\n",
            "Iteration 220, loss = 0.00722253\n",
            "Iteration 221, loss = 0.00706228\n",
            "Iteration 222, loss = 0.00733805\n",
            "Iteration 223, loss = 0.00678006\n",
            "Iteration 224, loss = 0.00701871\n",
            "Iteration 225, loss = 0.00679977\n",
            "Iteration 226, loss = 0.00682394\n",
            "Iteration 227, loss = 0.00659489\n",
            "Iteration 228, loss = 0.00660055\n",
            "Iteration 229, loss = 0.00654327\n",
            "Iteration 230, loss = 0.00654418\n",
            "Iteration 231, loss = 0.00641886\n",
            "Iteration 232, loss = 0.00637667\n",
            "Iteration 233, loss = 0.00647901\n",
            "Iteration 234, loss = 0.00627604\n",
            "Iteration 235, loss = 0.00643940\n",
            "Iteration 236, loss = 0.00627420\n",
            "Iteration 237, loss = 0.00637859\n",
            "Iteration 238, loss = 0.00655280\n",
            "Iteration 239, loss = 0.00624776\n",
            "Iteration 240, loss = 0.00639300\n",
            "Iteration 241, loss = 0.00599980\n",
            "Iteration 242, loss = 0.00608808\n",
            "Iteration 243, loss = 0.00598588\n",
            "Iteration 244, loss = 0.00596645\n",
            "Iteration 245, loss = 0.00587290\n",
            "Iteration 246, loss = 0.00591625\n",
            "Iteration 247, loss = 0.00609051\n",
            "Iteration 248, loss = 0.00574914\n",
            "Iteration 249, loss = 0.00593410\n",
            "Iteration 250, loss = 0.00571780\n",
            "Iteration 251, loss = 0.00586687\n",
            "Iteration 252, loss = 0.00603781\n",
            "Iteration 253, loss = 0.00557863\n",
            "Iteration 254, loss = 0.00587741\n",
            "Iteration 255, loss = 0.00586759\n",
            "Iteration 256, loss = 0.00556029\n",
            "Iteration 257, loss = 0.00547998\n",
            "Iteration 258, loss = 0.00563041\n",
            "Iteration 259, loss = 0.00549910\n",
            "Iteration 260, loss = 0.00577985\n",
            "Iteration 261, loss = 0.00556372\n",
            "Iteration 262, loss = 0.00593949\n",
            "Iteration 263, loss = 0.00587119\n",
            "Iteration 264, loss = 0.00565906\n",
            "Iteration 265, loss = 0.00558260\n",
            "Iteration 266, loss = 0.00533760\n",
            "Iteration 267, loss = 0.00550141\n",
            "Iteration 268, loss = 0.00560683\n",
            "Iteration 269, loss = 0.00515198\n",
            "Iteration 270, loss = 0.00530592\n",
            "Iteration 271, loss = 0.00525656\n",
            "Iteration 272, loss = 0.00512957\n",
            "Iteration 273, loss = 0.00503306\n",
            "Iteration 274, loss = 0.00552312\n",
            "Iteration 275, loss = 0.00517183\n",
            "Iteration 276, loss = 0.00505350\n",
            "Iteration 277, loss = 0.00511325\n",
            "Iteration 278, loss = 0.00490753\n",
            "Iteration 279, loss = 0.00510624\n",
            "Iteration 280, loss = 0.00526787\n",
            "Iteration 281, loss = 0.00503680\n",
            "Iteration 282, loss = 0.00483537\n",
            "Iteration 283, loss = 0.00481664\n",
            "Iteration 284, loss = 0.00472274\n",
            "Iteration 285, loss = 0.00480904\n",
            "Iteration 286, loss = 0.00489314\n",
            "Iteration 287, loss = 0.00483094\n",
            "Iteration 288, loss = 0.00476303\n",
            "Iteration 289, loss = 0.00457700\n",
            "Iteration 290, loss = 0.00465378\n",
            "Iteration 291, loss = 0.00461907\n",
            "Iteration 292, loss = 0.00468219\n",
            "Iteration 293, loss = 0.00472822\n",
            "Iteration 294, loss = 0.00459053\n",
            "Iteration 295, loss = 0.00454884\n",
            "Iteration 296, loss = 0.00446756\n",
            "Iteration 297, loss = 0.00471232\n",
            "Iteration 298, loss = 0.00446276\n",
            "Iteration 299, loss = 0.00449514\n",
            "Iteration 300, loss = 0.00436793\n",
            "Iteration 301, loss = 0.00435870\n",
            "Iteration 302, loss = 0.00423864\n",
            "Iteration 303, loss = 0.00431849\n",
            "Iteration 304, loss = 0.00425122\n",
            "Iteration 305, loss = 0.00418097\n",
            "Iteration 306, loss = 0.00412719\n",
            "Iteration 307, loss = 0.00419289\n",
            "Iteration 308, loss = 0.00414663\n",
            "Iteration 309, loss = 0.00403755\n",
            "Iteration 310, loss = 0.00405534\n",
            "Iteration 311, loss = 0.00405455\n",
            "Iteration 312, loss = 0.00413581\n",
            "Iteration 313, loss = 0.00403789\n",
            "Iteration 314, loss = 0.00403439\n",
            "Iteration 315, loss = 0.00396155\n",
            "Iteration 316, loss = 0.00390250\n",
            "Iteration 317, loss = 0.00405134\n",
            "Iteration 318, loss = 0.00408392\n",
            "Iteration 319, loss = 0.00393528\n",
            "Iteration 320, loss = 0.00391768\n",
            "Iteration 321, loss = 0.00394650\n",
            "Iteration 322, loss = 0.00390701\n",
            "Iteration 323, loss = 0.00380747\n",
            "Iteration 324, loss = 0.00387888\n",
            "Iteration 325, loss = 0.00386945\n",
            "Iteration 326, loss = 0.00378549\n",
            "Iteration 327, loss = 0.00386454\n",
            "Iteration 328, loss = 0.00376027\n",
            "Iteration 329, loss = 0.00369898\n",
            "Iteration 330, loss = 0.00373985\n",
            "Iteration 331, loss = 0.00378279\n",
            "Iteration 332, loss = 0.00366932\n",
            "Iteration 333, loss = 0.00369140\n",
            "Iteration 334, loss = 0.00371131\n",
            "Iteration 335, loss = 0.00387097\n",
            "Iteration 336, loss = 0.00389484\n",
            "Iteration 337, loss = 0.00364982\n",
            "Iteration 338, loss = 0.00372015\n",
            "Iteration 339, loss = 0.00347047\n",
            "Iteration 340, loss = 0.00345037\n",
            "Iteration 341, loss = 0.00346694\n",
            "Iteration 342, loss = 0.00362087\n",
            "Iteration 343, loss = 0.00341475\n",
            "Iteration 344, loss = 0.00374645\n",
            "Iteration 345, loss = 0.00329467\n",
            "Iteration 346, loss = 0.00338600\n",
            "Iteration 347, loss = 0.00351396\n",
            "Iteration 348, loss = 0.00344597\n",
            "Iteration 349, loss = 0.00364775\n",
            "Iteration 350, loss = 0.00344562\n",
            "Iteration 351, loss = 0.00331086\n",
            "Iteration 352, loss = 0.00330030\n",
            "Iteration 353, loss = 0.00336090\n",
            "Iteration 354, loss = 0.00329651\n",
            "Iteration 355, loss = 0.00329616\n",
            "Iteration 356, loss = 0.00345891\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(20, 100), max_iter=1000, tol=1e-05,\n",
              "              verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(20, 100), max_iter=1000, tol=1e-05,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(20, 100), max_iter=1000, tol=1e-05,\n",
              "              verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes = rede_neural_credit.predict(X_credit_teste)\n",
        "previsoes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsoU-r-Xsn3j",
        "outputId": "65fe6377-97fd-4322-dca8-9e771d8296ec"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_credit_teste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RqDqa2VtFzx",
        "outputId": "1a9cc46f-7a6f-487c-d9d2-cbb46b297d47"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "408     0\n",
              "1191    0\n",
              "677     0\n",
              "1360    1\n",
              "813     0\n",
              "       ..\n",
              "1900    0\n",
              "1524    0\n",
              "1415    0\n",
              "1306    0\n",
              "1767    0\n",
              "Name: default, Length: 500, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "accuracy_score(y_credit_teste, previsoes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tsc9BDePtwfg",
        "outputId": "d5166041-916f-4820-c5ad-30c2c7c5df6a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.994"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}